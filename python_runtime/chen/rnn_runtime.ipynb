{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-15 22:54:04,131, DEBUG, Start reading formatted input\n",
      "2017-12-15 22:54:05,148, DEBUG, End reading formatted input\n",
      "------------------------------------------------------------------------\n",
      "2017-12-15 22:54:05,149, DEBUG, Start RNN training master\n",
      "2017-12-15 22:54:06,681, DEBUG, exp id, 0, Start RNN worker\n",
      "2017-12-15 22:54:06,709, DEBUG, exp id, 1, Start RNN worker\n",
      "2017-12-15 22:54:06,745, DEBUG, exp id, 2, Start RNN worker\n",
      "2017-12-15 22:54:06,777, DEBUG, exp id, 3, Start RNN worker\n",
      "2017-12-15 22:54:06,928, DEBUG, exp id, 4, Start RNN worker\n",
      "2017-12-15 22:54:07,032, DEBUG, exp id, 5, Start RNN worker\n",
      "2017-12-15 22:54:07,150, DEBUG, exp id, 6, Start RNN worker\n",
      "2017-12-15 22:54:07,340, DEBUG, exp id, 7, Start RNN worker\n",
      "2017-12-15 22:54:07,540, DEBUG, exp id, 8, Start RNN worker\n",
      "2017-12-15 22:54:07,742, DEBUG, exp id, 9, Start RNN worker\n",
      "2017-12-15 22:55:39,941, DEBUG, exp id, 6, Step, 1, Minibatch Loss, 3.70231, Training Accuracy, 0.19531\n",
      "2017-12-15 22:55:42,327, DEBUG, exp id, 2, Step, 1, Minibatch Loss, 2.62539, Training Accuracy, 0.30078\n",
      "2017-12-15 22:55:46,639, DEBUG, exp id, 4, Step, 1, Minibatch Loss, 3.52442, Training Accuracy, 0.37500\n",
      "2017-12-15 22:55:46,936, DEBUG, exp id, 5, Step, 1, Minibatch Loss, 2.98828, Training Accuracy, 0.20312\n",
      "2017-12-15 22:55:56,929, DEBUG, exp id, 1, Step, 1, Minibatch Loss, 4.02324, Training Accuracy, 0.30469\n",
      "2017-12-15 22:55:57,626, DEBUG, exp id, 0, Step, 1, Minibatch Loss, 4.34355, Training Accuracy, 0.29688\n",
      "2017-12-15 22:55:57,626, DEBUG, exp id, 3, Step, 1, Minibatch Loss, 3.21672, Training Accuracy, 0.33398\n",
      "2017-12-15 22:55:59,927, DEBUG, exp id, 9, Step, 1, Minibatch Loss, 1.89694, Training Accuracy, 0.48438\n",
      "2017-12-15 22:55:59,927, DEBUG, exp id, 8, Step, 1, Minibatch Loss, 4.35620, Training Accuracy, 0.26562\n",
      "2017-12-15 22:56:00,026, DEBUG, exp id, 7, Step, 1, Minibatch Loss, 4.17979, Training Accuracy, 0.27344\n",
      "2017-12-15 22:56:15,441, DEBUG, exp id, 6, Step, 100, Minibatch Loss, 0.94169, Training Accuracy, 0.62109\n",
      "2017-12-15 22:56:19,731, DEBUG, exp id, 2, Step, 100, Minibatch Loss, 1.10319, Training Accuracy, 0.60156\n",
      "2017-12-15 22:56:24,540, DEBUG, exp id, 5, Step, 100, Minibatch Loss, 0.86292, Training Accuracy, 0.67969\n",
      "2017-12-15 22:56:25,737, DEBUG, exp id, 4, Step, 100, Minibatch Loss, 0.85613, Training Accuracy, 0.60938\n",
      "2017-12-15 22:56:40,025, DEBUG, exp id, 0, Step, 100, Minibatch Loss, 1.12588, Training Accuracy, 0.67188\n",
      "2017-12-15 22:56:40,433, DEBUG, exp id, 1, Step, 100, Minibatch Loss, 1.03307, Training Accuracy, 0.62500\n",
      "2017-12-15 22:56:42,226, DEBUG, exp id, 9, Step, 100, Minibatch Loss, 0.82691, Training Accuracy, 0.67188\n",
      "2017-12-15 22:56:42,526, DEBUG, exp id, 3, Step, 100, Minibatch Loss, 1.15616, Training Accuracy, 0.61133\n",
      "2017-12-15 22:56:43,032, DEBUG, exp id, 8, Step, 100, Minibatch Loss, 0.73626, Training Accuracy, 0.68750\n",
      "2017-12-15 22:56:46,025, DEBUG, exp id, 7, Step, 100, Minibatch Loss, 0.72857, Training Accuracy, 0.73438\n",
      "2017-12-15 22:56:58,226, DEBUG, exp id, 6, Step, 200, Minibatch Loss, 0.83802, Training Accuracy, 0.71484\n",
      "2017-12-15 22:57:02,526, DEBUG, exp id, 2, Step, 200, Minibatch Loss, 1.06029, Training Accuracy, 0.61719\n",
      "2017-12-15 22:57:05,825, DEBUG, exp id, 5, Step, 200, Minibatch Loss, 0.88148, Training Accuracy, 0.67188\n",
      "2017-12-15 22:57:08,031, DEBUG, exp id, 4, Step, 200, Minibatch Loss, 0.69274, Training Accuracy, 0.71875\n",
      "2017-12-15 22:57:24,937, DEBUG, exp id, 1, Step, 200, Minibatch Loss, 0.92890, Training Accuracy, 0.67188\n",
      "2017-12-15 22:57:25,326, DEBUG, exp id, 0, Step, 200, Minibatch Loss, 0.87599, Training Accuracy, 0.68750\n",
      "2017-12-15 22:57:28,536, DEBUG, exp id, 8, Step, 200, Minibatch Loss, 0.56667, Training Accuracy, 0.78125\n",
      "2017-12-15 22:57:28,639, DEBUG, exp id, 9, Step, 200, Minibatch Loss, 0.56306, Training Accuracy, 0.84375\n",
      "2017-12-15 22:57:29,832, DEBUG, exp id, 3, Step, 200, Minibatch Loss, 0.91502, Training Accuracy, 0.65820\n",
      "2017-12-15 22:57:33,538, DEBUG, exp id, 7, Step, 200, Minibatch Loss, 0.63099, Training Accuracy, 0.77930\n",
      "2017-12-15 22:57:40,448, DEBUG, exp id, 6, Step, 300, Minibatch Loss, 0.72359, Training Accuracy, 0.75000\n",
      "2017-12-15 22:57:44,930, DEBUG, exp id, 2, Step, 300, Minibatch Loss, 1.02593, Training Accuracy, 0.60547\n",
      "2017-12-15 22:57:49,326, DEBUG, exp id, 5, Step, 300, Minibatch Loss, 0.74403, Training Accuracy, 0.75000\n",
      "2017-12-15 22:57:51,629, DEBUG, exp id, 4, Step, 300, Minibatch Loss, 0.78562, Training Accuracy, 0.75000\n",
      "2017-12-15 22:58:07,433, DEBUG, exp id, 1, Step, 300, Minibatch Loss, 0.77036, Training Accuracy, 0.74219\n",
      "2017-12-15 22:58:08,843, DEBUG, exp id, 0, Step, 300, Minibatch Loss, 0.62370, Training Accuracy, 0.82812\n",
      "2017-12-15 22:58:11,026, DEBUG, exp id, 9, Step, 300, Minibatch Loss, 0.52131, Training Accuracy, 0.82031\n",
      "2017-12-15 22:58:11,429, DEBUG, exp id, 8, Step, 300, Minibatch Loss, 0.46495, Training Accuracy, 0.81250\n",
      "2017-12-15 22:58:14,942, DEBUG, exp id, 3, Step, 300, Minibatch Loss, 0.92751, Training Accuracy, 0.64648\n",
      "2017-12-15 22:58:18,729, DEBUG, exp id, 7, Step, 300, Minibatch Loss, 0.57729, Training Accuracy, 0.79883\n",
      "2017-12-15 22:58:23,537, DEBUG, exp id, 6, Step, 400, Minibatch Loss, 0.66672, Training Accuracy, 0.74609\n",
      "2017-12-15 22:58:27,840, DEBUG, exp id, 2, Step, 400, Minibatch Loss, 0.94785, Training Accuracy, 0.67188\n",
      "2017-12-15 22:58:31,437, DEBUG, exp id, 5, Step, 400, Minibatch Loss, 0.67979, Training Accuracy, 0.77344\n",
      "2017-12-15 22:58:33,429, DEBUG, exp id, 4, Step, 400, Minibatch Loss, 0.53696, Training Accuracy, 0.82812\n",
      "2017-12-15 22:58:49,932, DEBUG, exp id, 1, Step, 400, Minibatch Loss, 0.74361, Training Accuracy, 0.69531\n",
      "2017-12-15 22:58:52,726, DEBUG, exp id, 0, Step, 400, Minibatch Loss, 0.83541, Training Accuracy, 0.73438\n",
      "2017-12-15 22:58:55,238, DEBUG, exp id, 9, Step, 400, Minibatch Loss, 0.59138, Training Accuracy, 0.83594\n",
      "2017-12-15 22:58:55,639, DEBUG, exp id, 8, Step, 400, Minibatch Loss, 0.51152, Training Accuracy, 0.82812\n",
      "2017-12-15 22:59:02,441, DEBUG, exp id, 3, Step, 400, Minibatch Loss, 0.84863, Training Accuracy, 0.67773\n",
      "2017-12-15 22:59:05,935, DEBUG, exp id, 7, Step, 400, Minibatch Loss, 0.60228, Training Accuracy, 0.78516\n",
      "2017-12-15 22:59:06,128, DEBUG, exp id, 6, Step, 500, Minibatch Loss, 0.66177, Training Accuracy, 0.77734\n",
      "2017-12-15 22:59:10,140, DEBUG, exp id, 2, Step, 500, Minibatch Loss, 0.83063, Training Accuracy, 0.68359\n",
      "2017-12-15 22:59:13,127, DEBUG, exp id, 5, Step, 500, Minibatch Loss, 0.64200, Training Accuracy, 0.74219\n",
      "2017-12-15 22:59:14,943, DEBUG, exp id, 4, Step, 500, Minibatch Loss, 0.71736, Training Accuracy, 0.71875\n",
      "2017-12-15 22:59:31,525, DEBUG, exp id, 1, Step, 500, Minibatch Loss, 0.68037, Training Accuracy, 0.80469\n",
      "2017-12-15 22:59:36,239, DEBUG, exp id, 0, Step, 500, Minibatch Loss, 0.88965, Training Accuracy, 0.62500\n",
      "2017-12-15 22:59:38,828, DEBUG, exp id, 8, Step, 500, Minibatch Loss, 0.47065, Training Accuracy, 0.84375\n",
      "2017-12-15 22:59:39,225, DEBUG, exp id, 9, Step, 500, Minibatch Loss, 0.55504, Training Accuracy, 0.82812\n",
      "2017-12-15 22:59:47,529, DEBUG, exp id, 6, Step, 600, Minibatch Loss, 0.71867, Training Accuracy, 0.76172\n",
      "2017-12-15 22:59:49,125, DEBUG, exp id, 3, Step, 500, Minibatch Loss, 0.86319, Training Accuracy, 0.67188\n",
      "2017-12-15 22:59:50,439, DEBUG, exp id, 7, Step, 500, Minibatch Loss, 0.47055, Training Accuracy, 0.83789\n",
      "2017-12-15 22:59:52,331, DEBUG, exp id, 2, Step, 600, Minibatch Loss, 0.76456, Training Accuracy, 0.70703\n",
      "2017-12-15 22:59:54,042, DEBUG, exp id, 5, Step, 600, Minibatch Loss, 0.58165, Training Accuracy, 0.81250\n",
      "2017-12-15 22:59:55,928, DEBUG, exp id, 4, Step, 600, Minibatch Loss, 0.67551, Training Accuracy, 0.76562\n",
      "2017-12-15 23:00:13,737, DEBUG, exp id, 1, Step, 600, Minibatch Loss, 0.83499, Training Accuracy, 0.71875\n",
      "2017-12-15 23:00:19,834, DEBUG, exp id, 0, Step, 600, Minibatch Loss, 0.85662, Training Accuracy, 0.68750\n",
      "2017-12-15 23:00:23,139, DEBUG, exp id, 8, Step, 600, Minibatch Loss, 0.46143, Training Accuracy, 0.81250\n",
      "2017-12-15 23:00:24,027, DEBUG, exp id, 9, Step, 600, Minibatch Loss, 0.45362, Training Accuracy, 0.85156\n",
      "2017-12-15 23:00:29,639, DEBUG, exp id, 6, Step, 700, Minibatch Loss, 0.66203, Training Accuracy, 0.76953\n",
      "2017-12-15 23:00:35,725, DEBUG, exp id, 2, Step, 700, Minibatch Loss, 0.78790, Training Accuracy, 0.72656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-15 23:00:36,428, DEBUG, exp id, 5, Step, 700, Minibatch Loss, 0.64521, Training Accuracy, 0.82031\n",
      "2017-12-15 23:00:36,527, DEBUG, exp id, 3, Step, 600, Minibatch Loss, 0.75431, Training Accuracy, 0.72461\n",
      "2017-12-15 23:00:37,325, DEBUG, exp id, 7, Step, 600, Minibatch Loss, 0.39725, Training Accuracy, 0.87109\n",
      "2017-12-15 23:00:38,239, DEBUG, exp id, 4, Step, 700, Minibatch Loss, 0.55200, Training Accuracy, 0.81250\n",
      "2017-12-15 23:00:55,644, DEBUG, exp id, 1, Step, 700, Minibatch Loss, 0.77931, Training Accuracy, 0.69531\n",
      "2017-12-15 23:01:02,137, DEBUG, exp id, 0, Step, 700, Minibatch Loss, 1.08252, Training Accuracy, 0.67188\n",
      "2017-12-15 23:01:07,037, DEBUG, exp id, 9, Step, 700, Minibatch Loss, 0.37439, Training Accuracy, 0.89062\n",
      "2017-12-15 23:01:07,128, DEBUG, exp id, 8, Step, 700, Minibatch Loss, 0.52305, Training Accuracy, 0.81250\n",
      "2017-12-15 23:01:12,730, DEBUG, exp id, 6, Step, 800, Minibatch Loss, 0.52448, Training Accuracy, 0.83984\n",
      "2017-12-15 23:01:18,334, DEBUG, exp id, 5, Step, 800, Minibatch Loss, 0.54338, Training Accuracy, 0.80469\n",
      "2017-12-15 23:01:19,229, DEBUG, exp id, 2, Step, 800, Minibatch Loss, 0.59925, Training Accuracy, 0.81250\n",
      "2017-12-15 23:01:21,327, DEBUG, exp id, 4, Step, 800, Minibatch Loss, 0.41819, Training Accuracy, 0.90625\n",
      "2017-12-15 23:01:21,535, DEBUG, exp id, 3, Step, 700, Minibatch Loss, 0.77945, Training Accuracy, 0.70312\n",
      "2017-12-15 23:01:23,042, DEBUG, exp id, 7, Step, 700, Minibatch Loss, 0.46724, Training Accuracy, 0.85156\n",
      "2017-12-15 23:01:38,126, DEBUG, exp id, 1, Step, 800, Minibatch Loss, 0.77361, Training Accuracy, 0.71875\n",
      "2017-12-15 23:01:45,645, DEBUG, exp id, 0, Step, 800, Minibatch Loss, 0.59915, Training Accuracy, 0.79688\n",
      "2017-12-15 23:01:51,727, DEBUG, exp id, 9, Step, 800, Minibatch Loss, 0.45336, Training Accuracy, 0.86719\n",
      "2017-12-15 23:01:52,725, DEBUG, exp id, 8, Step, 800, Minibatch Loss, 0.44838, Training Accuracy, 0.85938\n",
      "2017-12-15 23:01:55,336, DEBUG, exp id, 6, Step, 900, Minibatch Loss, 0.52720, Training Accuracy, 0.83203\n",
      "2017-12-15 23:02:00,930, DEBUG, exp id, 5, Step, 900, Minibatch Loss, 0.54244, Training Accuracy, 0.82031\n",
      "2017-12-15 23:02:01,839, DEBUG, exp id, 2, Step, 900, Minibatch Loss, 0.72587, Training Accuracy, 0.76172\n",
      "2017-12-15 23:02:02,839, DEBUG, exp id, 4, Step, 900, Minibatch Loss, 0.47257, Training Accuracy, 0.87500\n",
      "2017-12-15 23:02:07,640, DEBUG, exp id, 3, Step, 800, Minibatch Loss, 0.70941, Training Accuracy, 0.74414\n",
      "2017-12-15 23:02:10,140, DEBUG, exp id, 7, Step, 800, Minibatch Loss, 0.42272, Training Accuracy, 0.88086\n",
      "2017-12-15 23:02:21,826, DEBUG, exp id, 1, Step, 900, Minibatch Loss, 0.86290, Training Accuracy, 0.69531\n",
      "2017-12-15 23:02:29,628, DEBUG, exp id, 0, Step, 900, Minibatch Loss, 0.67175, Training Accuracy, 0.75000\n",
      "2017-12-15 23:02:35,825, DEBUG, exp id, 9, Step, 900, Minibatch Loss, 0.48362, Training Accuracy, 0.84375\n",
      "2017-12-15 23:02:36,526, DEBUG, exp id, 8, Step, 900, Minibatch Loss, 0.54007, Training Accuracy, 0.82812\n",
      "2017-12-15 23:02:37,231, DEBUG, exp id, 6, Step, 1000, Minibatch Loss, 0.52366, Training Accuracy, 0.85547\n",
      "2017-12-15 23:02:42,226, DEBUG, exp id, 5, Step, 1000, Minibatch Loss, 0.53344, Training Accuracy, 0.81250\n",
      "2017-12-15 23:02:44,326, DEBUG, exp id, 2, Step, 1000, Minibatch Loss, 0.76349, Training Accuracy, 0.71094\n",
      "2017-12-15 23:02:44,625, DEBUG, exp id, 4, Step, 1000, Minibatch Loss, 0.67929, Training Accuracy, 0.71875\n",
      "2017-12-15 23:02:52,531, DEBUG, exp id, 3, Step, 900, Minibatch Loss, 0.73069, Training Accuracy, 0.75586\n",
      "2017-12-15 23:02:54,834, DEBUG, exp id, 7, Step, 900, Minibatch Loss, 0.44369, Training Accuracy, 0.84375\n",
      "2017-12-15 23:03:03,828, DEBUG, exp id, 1, Step, 1000, Minibatch Loss, 0.73185, Training Accuracy, 0.74219\n",
      "2017-12-15 23:03:11,645, DEBUG, exp id, 0, Step, 1000, Minibatch Loss, 0.59528, Training Accuracy, 0.89062\n",
      "2017-12-15 23:03:19,527, DEBUG, exp id, 6, Step, 1100, Minibatch Loss, 0.52308, Training Accuracy, 0.83594\n",
      "2017-12-15 23:03:19,530, DEBUG, exp id, 9, Step, 1000, Minibatch Loss, 0.40417, Training Accuracy, 0.85156\n",
      "2017-12-15 23:03:19,532, DEBUG, exp id, 8, Step, 1000, Minibatch Loss, 0.53446, Training Accuracy, 0.85938\n",
      "2017-12-15 23:03:24,530, DEBUG, exp id, 5, Step, 1100, Minibatch Loss, 0.51594, Training Accuracy, 0.86719\n",
      "2017-12-15 23:03:25,426, DEBUG, exp id, 2, Step, 1100, Minibatch Loss, 0.70346, Training Accuracy, 0.72266\n",
      "2017-12-15 23:03:25,526, DEBUG, exp id, 4, Step, 1100, Minibatch Loss, 0.44394, Training Accuracy, 0.84375\n",
      "2017-12-15 23:03:40,630, DEBUG, exp id, 3, Step, 1000, Minibatch Loss, 0.69655, Training Accuracy, 0.75781\n",
      "2017-12-15 23:03:42,628, DEBUG, exp id, 7, Step, 1000, Minibatch Loss, 0.34170, Training Accuracy, 0.90234\n",
      "2017-12-15 23:03:46,626, DEBUG, exp id, 1, Step, 1100, Minibatch Loss, 0.54149, Training Accuracy, 0.80469\n",
      "2017-12-15 23:03:57,737, DEBUG, exp id, 0, Step, 1100, Minibatch Loss, 0.68795, Training Accuracy, 0.73438\n",
      "2017-12-15 23:04:02,026, DEBUG, exp id, 6, Step, 1200, Minibatch Loss, 0.53155, Training Accuracy, 0.82812\n",
      "2017-12-15 23:04:04,625, DEBUG, exp id, 9, Step, 1100, Minibatch Loss, 0.37671, Training Accuracy, 0.92188\n",
      "2017-12-15 23:04:06,441, DEBUG, exp id, 8, Step, 1100, Minibatch Loss, 0.40177, Training Accuracy, 0.92188\n",
      "2017-12-15 23:04:07,044, DEBUG, exp id, 4, Step, 1200, Minibatch Loss, 0.56376, Training Accuracy, 0.81250\n",
      "2017-12-15 23:04:07,437, DEBUG, exp id, 5, Step, 1200, Minibatch Loss, 0.50341, Training Accuracy, 0.84375\n",
      "2017-12-15 23:04:07,826, DEBUG, exp id, 2, Step, 1200, Minibatch Loss, 0.70179, Training Accuracy, 0.77344\n",
      "2017-12-15 23:04:27,029, DEBUG, exp id, 3, Step, 1100, Minibatch Loss, 0.62876, Training Accuracy, 0.78125\n",
      "2017-12-15 23:04:27,741, DEBUG, exp id, 7, Step, 1100, Minibatch Loss, 0.38375, Training Accuracy, 0.88867\n",
      "2017-12-15 23:04:29,729, DEBUG, exp id, 1, Step, 1200, Minibatch Loss, 0.55985, Training Accuracy, 0.82812\n",
      "2017-12-15 23:04:40,133, DEBUG, exp id, 0, Step, 1200, Minibatch Loss, 0.58712, Training Accuracy, 0.71875\n",
      "2017-12-15 23:04:45,725, DEBUG, exp id, 6, Step, 1300, Minibatch Loss, 0.46256, Training Accuracy, 0.83203\n",
      "2017-12-15 23:04:47,128, DEBUG, exp id, 9, Step, 1200, Minibatch Loss, 0.27446, Training Accuracy, 0.94531\n",
      "2017-12-15 23:04:47,330, DEBUG, exp id, 4, Step, 1300, Minibatch Loss, 0.43496, Training Accuracy, 0.85938\n",
      "2017-12-15 23:04:48,334, DEBUG, exp id, 5, Step, 1300, Minibatch Loss, 0.53816, Training Accuracy, 0.84375\n",
      "2017-12-15 23:04:49,733, DEBUG, exp id, 8, Step, 1200, Minibatch Loss, 0.40511, Training Accuracy, 0.90625\n",
      "2017-12-15 23:04:50,226, DEBUG, exp id, 2, Step, 1300, Minibatch Loss, 0.62565, Training Accuracy, 0.76172\n",
      "2017-12-15 23:05:11,631, DEBUG, exp id, 1, Step, 1300, Minibatch Loss, 0.58206, Training Accuracy, 0.79688\n",
      "2017-12-15 23:05:14,826, DEBUG, exp id, 3, Step, 1200, Minibatch Loss, 0.66857, Training Accuracy, 0.73438\n",
      "2017-12-15 23:05:14,932, DEBUG, exp id, 7, Step, 1200, Minibatch Loss, 0.37703, Training Accuracy, 0.86328\n",
      "2017-12-15 23:05:25,126, DEBUG, exp id, 0, Step, 1300, Minibatch Loss, 0.56949, Training Accuracy, 0.78125\n",
      "2017-12-15 23:05:28,628, DEBUG, exp id, 4, Step, 1400, Minibatch Loss, 0.41470, Training Accuracy, 0.87500\n",
      "2017-12-15 23:05:28,928, DEBUG, exp id, 6, Step, 1400, Minibatch Loss, 0.48691, Training Accuracy, 0.83984\n",
      "2017-12-15 23:05:31,925, DEBUG, exp id, 5, Step, 1400, Minibatch Loss, 0.48801, Training Accuracy, 0.82812\n",
      "2017-12-15 23:05:32,938, DEBUG, exp id, 9, Step, 1300, Minibatch Loss, 0.41012, Training Accuracy, 0.86719\n",
      "2017-12-15 23:05:33,925, DEBUG, exp id, 2, Step, 1400, Minibatch Loss, 0.59686, Training Accuracy, 0.78906\n",
      "2017-12-15 23:05:35,130, DEBUG, exp id, 8, Step, 1300, Minibatch Loss, 0.44415, Training Accuracy, 0.84375\n",
      "2017-12-15 23:05:53,430, DEBUG, exp id, 1, Step, 1400, Minibatch Loss, 0.53624, Training Accuracy, 0.82031\n",
      "2017-12-15 23:05:58,925, DEBUG, exp id, 3, Step, 1300, Minibatch Loss, 0.62981, Training Accuracy, 0.77734\n",
      "2017-12-15 23:05:59,830, DEBUG, exp id, 7, Step, 1300, Minibatch Loss, 0.32973, Training Accuracy, 0.89453\n",
      "2017-12-15 23:06:07,728, DEBUG, exp id, 0, Step, 1400, Minibatch Loss, 0.53713, Training Accuracy, 0.82812\n",
      "2017-12-15 23:06:09,037, DEBUG, exp id, 4, Step, 1500, Minibatch Loss, 0.32924, Training Accuracy, 0.87500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-15 23:06:10,226, DEBUG, exp id, 6, Step, 1500, Minibatch Loss, 0.51785, Training Accuracy, 0.80859\n",
      "2017-12-15 23:06:12,527, DEBUG, exp id, 5, Step, 1500, Minibatch Loss, 0.44583, Training Accuracy, 0.87500\n",
      "2017-12-15 23:06:15,631, DEBUG, exp id, 2, Step, 1500, Minibatch Loss, 0.63264, Training Accuracy, 0.77344\n",
      "2017-12-15 23:06:15,742, DEBUG, exp id, 9, Step, 1400, Minibatch Loss, 0.42432, Training Accuracy, 0.89062\n",
      "2017-12-15 23:06:17,526, DEBUG, exp id, 8, Step, 1400, Minibatch Loss, 0.36260, Training Accuracy, 0.95312\n",
      "2017-12-15 23:06:35,639, DEBUG, exp id, 1, Step, 1500, Minibatch Loss, 0.57267, Training Accuracy, 0.79688\n",
      "2017-12-15 23:06:46,927, DEBUG, exp id, 3, Step, 1400, Minibatch Loss, 0.63149, Training Accuracy, 0.76758\n",
      "2017-12-15 23:06:47,732, DEBUG, exp id, 7, Step, 1400, Minibatch Loss, 0.39250, Training Accuracy, 0.85938\n",
      "2017-12-15 23:06:52,025, DEBUG, exp id, 4, Step, 1600, Minibatch Loss, 0.49631, Training Accuracy, 0.79688\n",
      "2017-12-15 23:06:52,938, DEBUG, exp id, 0, Step, 1500, Minibatch Loss, 0.47944, Training Accuracy, 0.85938\n",
      "2017-12-15 23:06:53,028, DEBUG, exp id, 6, Step, 1600, Minibatch Loss, 0.41502, Training Accuracy, 0.87109\n",
      "2017-12-15 23:06:55,133, DEBUG, exp id, 5, Step, 1600, Minibatch Loss, 0.48673, Training Accuracy, 0.82031\n",
      "2017-12-15 23:06:58,225, DEBUG, exp id, 2, Step, 1600, Minibatch Loss, 0.61511, Training Accuracy, 0.78125\n",
      "2017-12-15 23:07:01,042, DEBUG, exp id, 8, Step, 1500, Minibatch Loss, 0.40810, Training Accuracy, 0.87500\n",
      "2017-12-15 23:07:01,830, DEBUG, exp id, 9, Step, 1500, Minibatch Loss, 0.36253, Training Accuracy, 0.92188\n",
      "2017-12-15 23:07:18,029, DEBUG, exp id, 1, Step, 1600, Minibatch Loss, 0.53858, Training Accuracy, 0.78906\n",
      "2017-12-15 23:07:32,530, DEBUG, exp id, 4, Step, 1700, Minibatch Loss, 0.41790, Training Accuracy, 0.89062\n",
      "2017-12-15 23:07:33,241, DEBUG, exp id, 3, Step, 1500, Minibatch Loss, 0.68401, Training Accuracy, 0.75977\n",
      "2017-12-15 23:07:34,125, DEBUG, exp id, 7, Step, 1500, Minibatch Loss, 0.32892, Training Accuracy, 0.89062\n",
      "2017-12-15 23:07:35,330, DEBUG, exp id, 6, Step, 1700, Minibatch Loss, 0.42458, Training Accuracy, 0.85156\n",
      "2017-12-15 23:07:36,540, DEBUG, exp id, 5, Step, 1700, Minibatch Loss, 0.36160, Training Accuracy, 0.91406\n",
      "2017-12-15 23:07:36,949, DEBUG, exp id, 0, Step, 1600, Minibatch Loss, 0.50627, Training Accuracy, 0.92188\n",
      "2017-12-15 23:07:40,525, DEBUG, exp id, 2, Step, 1700, Minibatch Loss, 0.60535, Training Accuracy, 0.78125\n",
      "2017-12-15 23:07:44,732, DEBUG, exp id, 8, Step, 1600, Minibatch Loss, 0.35418, Training Accuracy, 0.93750\n",
      "2017-12-15 23:07:45,335, DEBUG, exp id, 9, Step, 1600, Minibatch Loss, 0.36531, Training Accuracy, 0.88281\n",
      "2017-12-15 23:08:00,533, DEBUG, exp id, 1, Step, 1700, Minibatch Loss, 0.47851, Training Accuracy, 0.82031\n",
      "2017-12-15 23:08:13,444, DEBUG, exp id, 4, Step, 1800, Minibatch Loss, 0.38659, Training Accuracy, 0.89062\n",
      "2017-12-15 23:08:18,042, DEBUG, exp id, 6, Step, 1800, Minibatch Loss, 0.39985, Training Accuracy, 0.86719\n",
      "2017-12-15 23:08:18,926, DEBUG, exp id, 5, Step, 1800, Minibatch Loss, 0.45772, Training Accuracy, 0.81250\n",
      "2017-12-15 23:08:19,526, DEBUG, exp id, 0, Step, 1700, Minibatch Loss, 0.59542, Training Accuracy, 0.81250\n",
      "2017-12-15 23:08:19,742, DEBUG, exp id, 3, Step, 1600, Minibatch Loss, 0.63281, Training Accuracy, 0.76367\n",
      "2017-12-15 23:08:21,041, DEBUG, exp id, 7, Step, 1600, Minibatch Loss, 0.30353, Training Accuracy, 0.89844\n",
      "2017-12-15 23:08:24,126, DEBUG, exp id, 2, Step, 1800, Minibatch Loss, 0.61742, Training Accuracy, 0.81641\n",
      "2017-12-15 23:08:29,428, DEBUG, exp id, 8, Step, 1700, Minibatch Loss, 0.31821, Training Accuracy, 0.89062\n",
      "2017-12-15 23:08:31,226, DEBUG, exp id, 9, Step, 1700, Minibatch Loss, 0.36637, Training Accuracy, 0.87500\n",
      "2017-12-15 23:08:42,631, DEBUG, exp id, 1, Step, 1800, Minibatch Loss, 0.57682, Training Accuracy, 0.74219\n",
      "2017-12-15 23:08:54,226, DEBUG, exp id, 4, Step, 1900, Minibatch Loss, 0.35593, Training Accuracy, 0.84375\n",
      "2017-12-15 23:09:00,037, DEBUG, exp id, 6, Step, 1900, Minibatch Loss, 0.46935, Training Accuracy, 0.83594\n",
      "2017-12-15 23:09:00,627, DEBUG, exp id, 5, Step, 1900, Minibatch Loss, 0.48493, Training Accuracy, 0.85156\n",
      "2017-12-15 23:09:03,344, DEBUG, exp id, 0, Step, 1800, Minibatch Loss, 0.40610, Training Accuracy, 0.85938\n",
      "2017-12-15 23:09:05,540, DEBUG, exp id, 3, Step, 1700, Minibatch Loss, 0.62932, Training Accuracy, 0.77344\n",
      "2017-12-15 23:09:06,231, DEBUG, exp id, 7, Step, 1700, Minibatch Loss, 0.34461, Training Accuracy, 0.88672\n",
      "2017-12-15 23:09:06,427, DEBUG, exp id, 2, Step, 1900, Minibatch Loss, 0.55157, Training Accuracy, 0.80859\n",
      "2017-12-15 23:09:11,330, DEBUG, exp id, 8, Step, 1800, Minibatch Loss, 0.34843, Training Accuracy, 0.87500\n",
      "2017-12-15 23:09:15,027, DEBUG, exp id, 9, Step, 1800, Minibatch Loss, 0.34788, Training Accuracy, 0.89062\n",
      "2017-12-15 23:09:24,228, DEBUG, exp id, 1, Step, 1900, Minibatch Loss, 0.62286, Training Accuracy, 0.77344\n",
      "2017-12-15 23:09:36,234, DEBUG, exp id, 4, Step, 2000, Minibatch Loss, 0.36824, Training Accuracy, 0.89062\n",
      "2017-12-15 23:09:43,028, DEBUG, exp id, 5, Step, 2000, Minibatch Loss, 0.55819, Training Accuracy, 0.81250\n",
      "2017-12-15 23:09:43,242, DEBUG, exp id, 6, Step, 2000, Minibatch Loss, 0.36941, Training Accuracy, 0.88281\n",
      "2017-12-15 23:09:47,242, DEBUG, exp id, 0, Step, 1900, Minibatch Loss, 0.57098, Training Accuracy, 0.82812\n",
      "2017-12-15 23:09:48,826, DEBUG, exp id, 2, Step, 2000, Minibatch Loss, 0.60878, Training Accuracy, 0.79297\n",
      "2017-12-15 23:09:52,941, DEBUG, exp id, 3, Step, 1800, Minibatch Loss, 0.65975, Training Accuracy, 0.76172\n",
      "2017-12-15 23:09:53,037, DEBUG, exp id, 7, Step, 1800, Minibatch Loss, 0.24294, Training Accuracy, 0.93750\n",
      "2017-12-15 23:09:56,627, DEBUG, exp id, 8, Step, 1900, Minibatch Loss, 0.33176, Training Accuracy, 0.95312\n",
      "2017-12-15 23:09:59,725, DEBUG, exp id, 9, Step, 1900, Minibatch Loss, 0.31093, Training Accuracy, 0.92188\n",
      "2017-12-15 23:10:06,825, DEBUG, exp id, 1, Step, 2000, Minibatch Loss, 0.54174, Training Accuracy, 0.81250\n",
      "2017-12-15 23:10:17,325, DEBUG, exp id, 4, Step, 2100, Minibatch Loss, 0.36304, Training Accuracy, 0.92188\n",
      "2017-12-15 23:10:24,331, DEBUG, exp id, 5, Step, 2100, Minibatch Loss, 0.48021, Training Accuracy, 0.85156\n",
      "2017-12-15 23:10:25,226, DEBUG, exp id, 6, Step, 2100, Minibatch Loss, 0.37598, Training Accuracy, 0.86719\n",
      "2017-12-15 23:10:29,625, DEBUG, exp id, 0, Step, 2000, Minibatch Loss, 0.68792, Training Accuracy, 0.78125\n",
      "2017-12-15 23:10:31,926, DEBUG, exp id, 2, Step, 2100, Minibatch Loss, 0.60613, Training Accuracy, 0.79688\n",
      "2017-12-15 23:10:38,742, DEBUG, exp id, 7, Step, 1900, Minibatch Loss, 0.26137, Training Accuracy, 0.93359\n",
      "2017-12-15 23:10:39,226, DEBUG, exp id, 3, Step, 1900, Minibatch Loss, 0.57039, Training Accuracy, 0.80469\n",
      "2017-12-15 23:10:39,526, DEBUG, exp id, 8, Step, 2000, Minibatch Loss, 0.27904, Training Accuracy, 0.87500\n",
      "2017-12-15 23:10:44,637, DEBUG, exp id, 9, Step, 2000, Minibatch Loss, 0.23065, Training Accuracy, 0.95312\n",
      "2017-12-15 23:10:49,033, DEBUG, exp id, 1, Step, 2100, Minibatch Loss, 0.57299, Training Accuracy, 0.76562\n",
      "2017-12-15 23:10:58,829, DEBUG, exp id, 4, Step, 2200, Minibatch Loss, 0.28136, Training Accuracy, 0.93750\n",
      "2017-12-15 23:11:05,725, DEBUG, exp id, 5, Step, 2200, Minibatch Loss, 0.39678, Training Accuracy, 0.90625\n",
      "2017-12-15 23:11:08,235, DEBUG, exp id, 6, Step, 2200, Minibatch Loss, 0.42468, Training Accuracy, 0.87500\n",
      "2017-12-15 23:11:12,026, DEBUG, exp id, 0, Step, 2100, Minibatch Loss, 0.53731, Training Accuracy, 0.78125\n",
      "2017-12-15 23:11:14,225, DEBUG, exp id, 2, Step, 2200, Minibatch Loss, 0.61111, Training Accuracy, 0.78906\n",
      "2017-12-15 23:11:24,026, DEBUG, exp id, 8, Step, 2100, Minibatch Loss, 0.28192, Training Accuracy, 0.90625\n",
      "2017-12-15 23:11:25,929, DEBUG, exp id, 3, Step, 2000, Minibatch Loss, 0.57865, Training Accuracy, 0.79883\n",
      "2017-12-15 23:11:26,326, DEBUG, exp id, 7, Step, 2000, Minibatch Loss, 0.32436, Training Accuracy, 0.89844\n",
      "2017-12-15 23:11:30,426, DEBUG, exp id, 9, Step, 2100, Minibatch Loss, 0.25912, Training Accuracy, 0.92188\n",
      "2017-12-15 23:11:30,928, DEBUG, exp id, 1, Step, 2200, Minibatch Loss, 0.49545, Training Accuracy, 0.83594\n",
      "2017-12-15 23:11:40,525, DEBUG, exp id, 4, Step, 2300, Minibatch Loss, 0.42656, Training Accuracy, 0.79688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-15 23:11:47,729, DEBUG, exp id, 5, Step, 2300, Minibatch Loss, 0.39516, Training Accuracy, 0.88281\n",
      "2017-12-15 23:11:50,528, DEBUG, exp id, 6, Step, 2300, Minibatch Loss, 0.41250, Training Accuracy, 0.86719\n",
      "2017-12-15 23:11:56,627, DEBUG, exp id, 2, Step, 2300, Minibatch Loss, 0.58607, Training Accuracy, 0.78516\n",
      "2017-12-15 23:11:57,538, DEBUG, exp id, 0, Step, 2200, Minibatch Loss, 0.59545, Training Accuracy, 0.75000\n",
      "2017-12-15 23:12:09,331, DEBUG, exp id, 8, Step, 2200, Minibatch Loss, 0.37208, Training Accuracy, 0.89062\n",
      "2017-12-15 23:12:13,026, DEBUG, exp id, 1, Step, 2300, Minibatch Loss, 0.49425, Training Accuracy, 0.85156\n",
      "2017-12-15 23:12:14,529, DEBUG, exp id, 3, Step, 2100, Minibatch Loss, 0.53099, Training Accuracy, 0.81250\n",
      "2017-12-15 23:12:14,729, DEBUG, exp id, 7, Step, 2100, Minibatch Loss, 0.29907, Training Accuracy, 0.91797\n",
      "2017-12-15 23:12:15,029, DEBUG, exp id, 9, Step, 2200, Minibatch Loss, 0.29485, Training Accuracy, 0.88281\n",
      "2017-12-15 23:12:22,425, DEBUG, exp id, 4, Step, 2400, Minibatch Loss, 0.29757, Training Accuracy, 0.90625\n",
      "2017-12-15 23:12:30,837, DEBUG, exp id, 5, Step, 2400, Minibatch Loss, 0.32036, Training Accuracy, 0.92188\n",
      "2017-12-15 23:12:33,234, DEBUG, exp id, 6, Step, 2400, Minibatch Loss, 0.45985, Training Accuracy, 0.83594\n",
      "2017-12-15 23:12:41,034, DEBUG, exp id, 2, Step, 2400, Minibatch Loss, 0.51136, Training Accuracy, 0.83203\n",
      "2017-12-15 23:12:41,831, DEBUG, exp id, 0, Step, 2300, Minibatch Loss, 0.53453, Training Accuracy, 0.78125\n",
      "2017-12-15 23:12:55,033, DEBUG, exp id, 8, Step, 2300, Minibatch Loss, 0.28727, Training Accuracy, 0.93750\n",
      "2017-12-15 23:12:56,128, DEBUG, exp id, 1, Step, 2400, Minibatch Loss, 0.53173, Training Accuracy, 0.82812\n",
      "2017-12-15 23:12:59,531, DEBUG, exp id, 9, Step, 2300, Minibatch Loss, 0.23040, Training Accuracy, 0.93750\n",
      "2017-12-15 23:13:01,828, DEBUG, exp id, 3, Step, 2200, Minibatch Loss, 0.59060, Training Accuracy, 0.77734\n",
      "2017-12-15 23:13:02,130, DEBUG, exp id, 7, Step, 2200, Minibatch Loss, 0.26193, Training Accuracy, 0.92188\n",
      "2017-12-15 23:13:04,129, DEBUG, exp id, 4, Step, 2500, Minibatch Loss, 0.39886, Training Accuracy, 0.89062\n",
      "2017-12-15 23:13:16,426, DEBUG, exp id, 5, Step, 2500, Minibatch Loss, 0.33361, Training Accuracy, 0.91406\n",
      "2017-12-15 23:13:17,933, DEBUG, exp id, 6, Step, 2500, Minibatch Loss, 0.40018, Training Accuracy, 0.86328\n",
      "2017-12-15 23:13:24,129, DEBUG, exp id, 0, Step, 2400, Minibatch Loss, 0.42045, Training Accuracy, 0.87500\n",
      "2017-12-15 23:13:24,130, DEBUG, exp id, 2, Step, 2500, Minibatch Loss, 0.59604, Training Accuracy, 0.77734\n",
      "2017-12-15 23:13:37,631, DEBUG, exp id, 1, Step, 2500, Minibatch Loss, 0.55089, Training Accuracy, 0.82812\n",
      "2017-12-15 23:13:38,830, DEBUG, exp id, 8, Step, 2400, Minibatch Loss, 0.35126, Training Accuracy, 0.89062\n",
      "2017-12-15 23:13:42,326, DEBUG, exp id, 9, Step, 2400, Minibatch Loss, 0.29913, Training Accuracy, 0.92188\n",
      "2017-12-15 23:13:45,330, DEBUG, exp id, 4, Step, 2600, Minibatch Loss, 0.32252, Training Accuracy, 0.90625\n",
      "2017-12-15 23:13:46,929, DEBUG, exp id, 7, Step, 2300, Minibatch Loss, 0.25917, Training Accuracy, 0.92578\n",
      "2017-12-15 23:13:47,542, DEBUG, exp id, 3, Step, 2300, Minibatch Loss, 0.57596, Training Accuracy, 0.79297\n",
      "2017-12-15 23:14:01,438, DEBUG, exp id, 5, Step, 2600, Minibatch Loss, 0.40507, Training Accuracy, 0.87500\n",
      "2017-12-15 23:14:02,330, DEBUG, exp id, 6, Step, 2600, Minibatch Loss, 0.38714, Training Accuracy, 0.88281\n",
      "2017-12-15 23:14:06,741, DEBUG, exp id, 0, Step, 2500, Minibatch Loss, 0.56841, Training Accuracy, 0.78125\n",
      "2017-12-15 23:14:07,228, DEBUG, exp id, 2, Step, 2600, Minibatch Loss, 0.52236, Training Accuracy, 0.82422\n",
      "2017-12-15 23:14:19,525, DEBUG, exp id, 1, Step, 2600, Minibatch Loss, 0.57128, Training Accuracy, 0.78125\n",
      "2017-12-15 23:14:23,229, DEBUG, exp id, 8, Step, 2500, Minibatch Loss, 0.19274, Training Accuracy, 0.98438\n",
      "2017-12-15 23:14:25,027, DEBUG, exp id, 4, Step, 2700, Minibatch Loss, 0.37666, Training Accuracy, 0.82812\n",
      "2017-12-15 23:14:27,938, DEBUG, exp id, 9, Step, 2500, Minibatch Loss, 0.28224, Training Accuracy, 0.92969\n",
      "2017-12-15 23:14:33,227, DEBUG, exp id, 7, Step, 2400, Minibatch Loss, 0.22700, Training Accuracy, 0.93750\n",
      "2017-12-15 23:14:34,831, DEBUG, exp id, 3, Step, 2400, Minibatch Loss, 0.55109, Training Accuracy, 0.80469\n",
      "2017-12-15 23:14:46,326, DEBUG, exp id, 5, Step, 2700, Minibatch Loss, 0.33520, Training Accuracy, 0.88281\n",
      "2017-12-15 23:14:47,531, DEBUG, exp id, 6, Step, 2700, Minibatch Loss, 0.31429, Training Accuracy, 0.90625\n",
      "2017-12-15 23:14:50,230, DEBUG, exp id, 0, Step, 2600, Minibatch Loss, 0.55312, Training Accuracy, 0.75000\n",
      "2017-12-15 23:14:50,432, DEBUG, exp id, 2, Step, 2700, Minibatch Loss, 0.57330, Training Accuracy, 0.79688\n",
      "2017-12-15 23:15:01,229, DEBUG, exp id, 1, Step, 2700, Minibatch Loss, 0.50687, Training Accuracy, 0.79688\n",
      "2017-12-15 23:15:05,637, DEBUG, exp id, 4, Step, 2800, Minibatch Loss, 0.32798, Training Accuracy, 0.93750\n",
      "2017-12-15 23:15:06,129, DEBUG, exp id, 8, Step, 2600, Minibatch Loss, 0.32685, Training Accuracy, 0.90625\n",
      "2017-12-15 23:15:13,127, DEBUG, exp id, 9, Step, 2600, Minibatch Loss, 0.26979, Training Accuracy, 0.91406\n",
      "2017-12-15 23:15:20,243, DEBUG, exp id, 7, Step, 2500, Minibatch Loss, 0.30127, Training Accuracy, 0.89453\n",
      "2017-12-15 23:15:20,935, DEBUG, exp id, 3, Step, 2500, Minibatch Loss, 0.56064, Training Accuracy, 0.79688\n",
      "2017-12-15 23:15:31,728, DEBUG, exp id, 6, Step, 2800, Minibatch Loss, 0.39176, Training Accuracy, 0.87109\n",
      "2017-12-15 23:15:32,133, DEBUG, exp id, 5, Step, 2800, Minibatch Loss, 0.31957, Training Accuracy, 0.89844\n",
      "2017-12-15 23:15:32,829, DEBUG, exp id, 2, Step, 2800, Minibatch Loss, 0.60220, Training Accuracy, 0.79688\n",
      "2017-12-15 23:15:33,431, DEBUG, exp id, 0, Step, 2700, Minibatch Loss, 0.41139, Training Accuracy, 0.90625\n",
      "2017-12-15 23:15:43,533, DEBUG, exp id, 1, Step, 2800, Minibatch Loss, 0.48893, Training Accuracy, 0.83594\n",
      "2017-12-15 23:15:47,425, DEBUG, exp id, 4, Step, 2900, Minibatch Loss, 0.32585, Training Accuracy, 0.89062\n",
      "2017-12-15 23:15:50,226, DEBUG, exp id, 8, Step, 2700, Minibatch Loss, 0.34822, Training Accuracy, 0.87500\n",
      "2017-12-15 23:15:59,126, DEBUG, exp id, 9, Step, 2700, Minibatch Loss, 0.20926, Training Accuracy, 0.95312\n",
      "2017-12-15 23:16:07,039, DEBUG, exp id, 7, Step, 2600, Minibatch Loss, 0.24792, Training Accuracy, 0.93555\n",
      "2017-12-15 23:16:07,629, DEBUG, exp id, 3, Step, 2600, Minibatch Loss, 0.52834, Training Accuracy, 0.81641\n",
      "2017-12-15 23:16:18,539, DEBUG, exp id, 5, Step, 2900, Minibatch Loss, 0.33498, Training Accuracy, 0.89844\n",
      "2017-12-15 23:16:18,725, DEBUG, exp id, 6, Step, 2900, Minibatch Loss, 0.39557, Training Accuracy, 0.89844\n",
      "2017-12-15 23:16:18,726, DEBUG, exp id, 2, Step, 2900, Minibatch Loss, 0.51277, Training Accuracy, 0.80078\n",
      "2017-12-15 23:16:19,036, DEBUG, exp id, 0, Step, 2800, Minibatch Loss, 0.43681, Training Accuracy, 0.82812\n",
      "2017-12-15 23:16:25,229, DEBUG, exp id, 1, Step, 2900, Minibatch Loss, 0.50637, Training Accuracy, 0.81250\n",
      "2017-12-15 23:16:32,126, DEBUG, exp id, 4, Step, 3000, Minibatch Loss, 0.54337, Training Accuracy, 0.81250\n",
      "2017-12-15 23:16:35,428, DEBUG, exp id, 8, Step, 2800, Minibatch Loss, 0.27487, Training Accuracy, 0.92188\n",
      "2017-12-15 23:16:42,940, DEBUG, exp id, 9, Step, 2800, Minibatch Loss, 0.18832, Training Accuracy, 0.96875\n",
      "2017-12-15 23:16:54,625, DEBUG, exp id, 3, Step, 2700, Minibatch Loss, 0.55821, Training Accuracy, 0.80469\n",
      "2017-12-15 23:16:55,231, DEBUG, exp id, 7, Step, 2700, Minibatch Loss, 0.26471, Training Accuracy, 0.91602\n",
      "2017-12-15 23:17:03,026, DEBUG, exp id, 2, Step, 3000, Minibatch Loss, 0.49786, Training Accuracy, 0.85156\n",
      "2017-12-15 23:17:04,642, DEBUG, exp id, 0, Step, 2900, Minibatch Loss, 0.57724, Training Accuracy, 0.79688\n",
      "2017-12-15 23:17:06,031, DEBUG, exp id, 1, Step, 3000, Minibatch Loss, 0.64730, Training Accuracy, 0.78125\n",
      "2017-12-15 23:17:06,335, DEBUG, exp id, 6, Step, 3000, Minibatch Loss, 0.35852, Training Accuracy, 0.89062\n",
      "2017-12-15 23:17:06,438, DEBUG, exp id, 5, Step, 3000, Minibatch Loss, 0.32112, Training Accuracy, 0.89844\n",
      "2017-12-15 23:17:17,435, DEBUG, exp id, 4, Step, 3100, Minibatch Loss, 0.38161, Training Accuracy, 0.85938\n",
      "2017-12-15 23:17:19,241, DEBUG, exp id, 8, Step, 2900, Minibatch Loss, 0.24766, Training Accuracy, 0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-15 23:17:27,828, DEBUG, exp id, 9, Step, 2900, Minibatch Loss, 0.18484, Training Accuracy, 0.95312\n",
      "2017-12-15 23:17:41,726, DEBUG, exp id, 3, Step, 2800, Minibatch Loss, 0.50211, Training Accuracy, 0.81836\n",
      "2017-12-15 23:17:43,141, DEBUG, exp id, 7, Step, 2800, Minibatch Loss, 0.23614, Training Accuracy, 0.94141\n",
      "2017-12-15 23:17:46,730, DEBUG, exp id, 1, Step, 3100, Minibatch Loss, 0.41894, Training Accuracy, 0.85156\n",
      "2017-12-15 23:17:47,541, DEBUG, exp id, 2, Step, 3100, Minibatch Loss, 0.55902, Training Accuracy, 0.84766\n",
      "2017-12-15 23:17:49,127, DEBUG, exp id, 0, Step, 3000, Minibatch Loss, 0.30015, Training Accuracy, 0.95312\n",
      "2017-12-15 23:17:51,736, DEBUG, exp id, 6, Step, 3100, Minibatch Loss, 0.29770, Training Accuracy, 0.91016\n",
      "2017-12-15 23:17:51,840, DEBUG, exp id, 5, Step, 3100, Minibatch Loss, 0.38477, Training Accuracy, 0.85938\n",
      "2017-12-15 23:18:01,130, DEBUG, exp id, 4, Step, 3200, Minibatch Loss, 0.49366, Training Accuracy, 0.84375\n",
      "2017-12-15 23:18:04,326, DEBUG, exp id, 8, Step, 3000, Minibatch Loss, 0.19870, Training Accuracy, 0.93750\n",
      "2017-12-15 23:18:13,127, DEBUG, exp id, 9, Step, 3000, Minibatch Loss, 0.20568, Training Accuracy, 0.94531\n",
      "2017-12-15 23:18:27,929, DEBUG, exp id, 1, Step, 3200, Minibatch Loss, 0.39018, Training Accuracy, 0.86719\n",
      "2017-12-15 23:18:29,325, DEBUG, exp id, 3, Step, 2900, Minibatch Loss, 0.53467, Training Accuracy, 0.80859\n",
      "2017-12-15 23:18:30,633, DEBUG, exp id, 7, Step, 2900, Minibatch Loss, 0.20888, Training Accuracy, 0.94336\n",
      "2017-12-15 23:18:33,143, DEBUG, exp id, 0, Step, 3100, Minibatch Loss, 0.44971, Training Accuracy, 0.84375\n",
      "2017-12-15 23:18:34,840, DEBUG, exp id, 2, Step, 3200, Minibatch Loss, 0.51435, Training Accuracy, 0.82031\n",
      "2017-12-15 23:18:38,730, DEBUG, exp id, 6, Step, 3200, Minibatch Loss, 0.29631, Training Accuracy, 0.91797\n",
      "2017-12-15 23:18:38,933, DEBUG, exp id, 5, Step, 3200, Minibatch Loss, 0.33061, Training Accuracy, 0.90625\n",
      "2017-12-15 23:18:48,034, DEBUG, exp id, 4, Step, 3300, Minibatch Loss, 0.38121, Training Accuracy, 0.90625\n",
      "2017-12-15 23:18:50,527, DEBUG, exp id, 8, Step, 3100, Minibatch Loss, 0.22823, Training Accuracy, 0.93750\n",
      "2017-12-15 23:18:59,126, DEBUG, exp id, 9, Step, 3100, Minibatch Loss, 0.21714, Training Accuracy, 0.94531\n",
      "2017-12-15 23:19:08,925, DEBUG, exp id, 1, Step, 3300, Minibatch Loss, 0.46495, Training Accuracy, 0.82031\n",
      "2017-12-15 23:19:16,638, DEBUG, exp id, 3, Step, 3000, Minibatch Loss, 0.49389, Training Accuracy, 0.81445\n",
      "2017-12-15 23:19:18,441, DEBUG, exp id, 0, Step, 3200, Minibatch Loss, 0.40821, Training Accuracy, 0.87500\n",
      "2017-12-15 23:19:18,529, DEBUG, exp id, 7, Step, 3000, Minibatch Loss, 0.17394, Training Accuracy, 0.95117\n",
      "2017-12-15 23:19:21,436, DEBUG, exp id, 2, Step, 3300, Minibatch Loss, 0.56373, Training Accuracy, 0.79688\n",
      "2017-12-15 23:19:26,228, DEBUG, exp id, 6, Step, 3300, Minibatch Loss, 0.34269, Training Accuracy, 0.92969\n",
      "2017-12-15 23:19:26,430, DEBUG, exp id, 5, Step, 3300, Minibatch Loss, 0.34244, Training Accuracy, 0.90625\n",
      "2017-12-15 23:19:32,132, DEBUG, exp id, 4, Step, 3400, Minibatch Loss, 0.30153, Training Accuracy, 0.89062\n",
      "2017-12-15 23:19:35,929, DEBUG, exp id, 8, Step, 3200, Minibatch Loss, 0.31606, Training Accuracy, 0.90625\n",
      "2017-12-15 23:19:45,141, DEBUG, exp id, 9, Step, 3200, Minibatch Loss, 0.13650, Training Accuracy, 0.96094\n",
      "2017-12-15 23:19:49,733, DEBUG, exp id, 1, Step, 3400, Minibatch Loss, 0.52446, Training Accuracy, 0.83594\n",
      "2017-12-15 23:20:03,325, DEBUG, exp id, 0, Step, 3300, Minibatch Loss, 0.52905, Training Accuracy, 0.73438\n",
      "2017-12-15 23:20:03,728, DEBUG, exp id, 3, Step, 3100, Minibatch Loss, 0.55090, Training Accuracy, 0.79883\n",
      "2017-12-15 23:20:06,344, DEBUG, exp id, 7, Step, 3100, Minibatch Loss, 0.23554, Training Accuracy, 0.94336\n",
      "2017-12-15 23:20:06,426, DEBUG, exp id, 2, Step, 3400, Minibatch Loss, 0.51988, Training Accuracy, 0.82812\n",
      "2017-12-15 23:20:12,733, DEBUG, exp id, 5, Step, 3400, Minibatch Loss, 0.25549, Training Accuracy, 0.95312\n",
      "2017-12-15 23:20:13,042, DEBUG, exp id, 6, Step, 3400, Minibatch Loss, 0.33240, Training Accuracy, 0.89453\n",
      "2017-12-15 23:20:16,431, DEBUG, exp id, 4, Step, 3500, Minibatch Loss, 0.25204, Training Accuracy, 0.92188\n",
      "2017-12-15 23:20:20,433, DEBUG, exp id, 8, Step, 3300, Minibatch Loss, 0.26105, Training Accuracy, 0.96875\n",
      "2017-12-15 23:20:29,827, DEBUG, exp id, 1, Step, 3500, Minibatch Loss, 0.48233, Training Accuracy, 0.84375\n",
      "2017-12-15 23:20:30,031, DEBUG, exp id, 9, Step, 3300, Minibatch Loss, 0.17346, Training Accuracy, 0.96094\n",
      "2017-12-15 23:20:47,629, DEBUG, exp id, 0, Step, 3400, Minibatch Loss, 0.58879, Training Accuracy, 0.78125\n",
      "2017-12-15 23:20:49,839, DEBUG, exp id, 2, Step, 3500, Minibatch Loss, 0.57131, Training Accuracy, 0.79688\n",
      "2017-12-15 23:20:51,630, DEBUG, exp id, 3, Step, 3200, Minibatch Loss, 0.53888, Training Accuracy, 0.81445\n",
      "2017-12-15 23:20:53,826, DEBUG, exp id, 7, Step, 3200, Minibatch Loss, 0.18714, Training Accuracy, 0.96484\n",
      "2017-12-15 23:20:58,541, DEBUG, exp id, 5, Step, 3500, Minibatch Loss, 0.45429, Training Accuracy, 0.82812\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import Queue\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "# constant for logger\n",
    "CURRENT_TIMESTAMP = datetime.datetime.fromtimestamp(time.time()).strftime('%Y_%m_%d_%H_%M_%S')\n",
    "RESULT_OUTPUT = 'run_rnn_mt_result_' + CURRENT_TIMESTAMP + '.log'\n",
    "LOGGER_FORMAT_HEADER = '%(asctime)s, %(levelname)s, %(message)s'\n",
    "CUTOFF_LINE = '------------------------------------------------------------------------'\n",
    "IS_ENABLE_FILE_LOGGING = True\n",
    "\n",
    "# general constant\n",
    "SPLIT_RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.33\n",
    "\n",
    "# constant for mt\n",
    "ENABLE_SIMPLE_RUN = False\n",
    "THREAD_COUNT = 10\n",
    "SIMPLE_LEARNING_LEN = 1\n",
    "SIMPLE_BATCH_LEN = 1\n",
    "SIMPLE_HIDDEN_LEN = 1\n",
    "\n",
    "\n",
    "# constant for rnn training\n",
    "# learning_rate = 0.001\n",
    "LEARNING_RATE_RANGE = [0.001, 0.005, 0.01]\n",
    "training_steps = 40000 # TODO: change the steps to 10000 for better result\n",
    "# batch_size = 128\n",
    "BATCH_SIZE_RANGE = [64, 128, 256, 512]\n",
    "display_step = 100\n",
    "\n",
    "# constant rnn network parameters\n",
    "num_input = 3 # we only read one set of yaw pitch row\n",
    "timesteps = 100  # timesteps - we have 100 data point for each char\n",
    "# num_hidden = 128  # hidden layer num of features\n",
    "NUM_HIDDEN_RANGE = [64, 128, 256, 512]\n",
    "num_classes = 5  # number of data class - using a/b/c/d/e\n",
    "\n",
    "# raw data file names\n",
    "DATA_SET_A = 'run_letter_a_format.csv'\n",
    "DATA_SET_B = 'run_letter_b_format.csv'\n",
    "DATA_SET_C = 'run_letter_c_format.csv'\n",
    "DATA_SET_D = 'run_letter_d_format.csv'\n",
    "DATA_SET_E = 'run_letter_e_format.csv'\n",
    "\n",
    "\n",
    "# bookkeeping logic for setup logger and random sample generator\n",
    "LOGGER = logging.getLogger('cogs181_runtime')\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(LOGGER_FORMAT_HEADER)\n",
    "\n",
    "# setup file logging if user enable\n",
    "if IS_ENABLE_FILE_LOGGING:\n",
    "    fh = logging.FileHandler(RESULT_OUTPUT)\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "    LOGGER.addHandler(fh)\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "LOGGER.addHandler(ch)\n",
    "\n",
    "random.seed(SPLIT_RANDOM_STATE)\n",
    "\n",
    "\n",
    "def read_format_input(read_file_name):\n",
    "    with open(read_file_name, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        raw_data_list = list(reader)\n",
    "\n",
    "    return raw_data_list\n",
    "\n",
    "\n",
    "def render_raw_data():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    raw_a_x = np.array(read_format_input(DATA_SET_A)).astype(None)\n",
    "    raw_b_x = np.array(read_format_input(DATA_SET_B)).astype(None)\n",
    "    raw_c_x = np.array(read_format_input(DATA_SET_C)).astype(None)\n",
    "    raw_d_x = np.array(read_format_input(DATA_SET_D)).astype(None)\n",
    "    raw_e_x = np.array(read_format_input(DATA_SET_E)).astype(None)\n",
    "    raw_x = np.concatenate((raw_a_x, raw_b_x, raw_c_x, raw_d_x, raw_e_x), axis=0)\n",
    "\n",
    "    raw_a_y = np.array([[1, 0, 0, 0, 0]] * len(raw_a_x)).astype(None)\n",
    "    raw_b_y = np.array([[0, 1, 0, 0, 0]] * len(raw_b_x)).astype(None)\n",
    "    raw_c_y = np.array([[0, 0, 1, 0, 0]] * len(raw_c_x)).astype(None)\n",
    "    raw_d_y = np.array([[0, 0, 0, 1, 0]] * len(raw_d_x)).astype(None)\n",
    "    raw_e_y = np.array([[0, 0, 0, 0, 1]] * len(raw_e_x)).astype(None)\n",
    "    raw_y = np.concatenate((raw_a_y, raw_b_y, raw_c_y, raw_d_y, raw_e_y), axis=0)\n",
    "\n",
    "    train_x, test_x, train_y, test_y = \\\n",
    "        train_test_split(raw_x, raw_y, test_size=TEST_SIZE, random_state=SPLIT_RANDOM_STATE)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "# TODO: make render_batch generate next set of unique batch instead of repeat some same index\n",
    "def render_batch(batch_size, x_data, y_data):\n",
    "    if len(x_data) != len(y_data):\n",
    "        sys.exit(\"Error: cannot render batch with different len of x and y.\")\n",
    "\n",
    "    batch_index = random.sample(range(0, len(x_data)), batch_size)\n",
    "    render_x = []\n",
    "    render_y = []\n",
    "\n",
    "    for index in batch_index:\n",
    "        render_x.append(x_data[index])\n",
    "        render_y.append(y_data[index])\n",
    "\n",
    "    render_x = np.array(render_x).astype(None)\n",
    "    render_y = np.array(render_y).astype(None)\n",
    "    return render_x, render_y\n",
    "\n",
    "\n",
    "def rnn_training_engine_worker(exp_id, train_x, train_y, test_x, test_y, layer_index, learning_index, batch_index):\n",
    "    def get_param(num_hidden, ):\n",
    "        import tensorflow as tf\n",
    "        # tf Graph input\n",
    "        X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "        Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "        weights = {\n",
    "            'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "        }\n",
    "\n",
    "        return X, Y, weights, biases\n",
    "\n",
    "    def rnn_nodes(x, weights, biases, num_hidden):\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.contrib import rnn\n",
    "        with tf.variable_scope('scope', reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            def lstm_cell():\n",
    "                return rnn.BasicLSTMCell(num_hidden, forget_bias=1.0, reuse=tf.get_variable_scope().reuse)\n",
    "\n",
    "            # Prepare data shape to match `rnn` function requirements\n",
    "            # Current data input shape: (batch_size, timesteps, n_input)\n",
    "            # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "            # unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "            x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "            # get lstm cell output\n",
    "            outputs, states = rnn.static_rnn([lstm_cell()][0], x, dtype=tf.float32)\n",
    "\n",
    "            # linear activation, using rnn inner loop last output\n",
    "            return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "    LOGGER.debug(\"exp id, \" + str(exp_id) + \", Start RNN worker\")\n",
    "\n",
    "    X, Y, weights, biases = get_param(NUM_HIDDEN_RANGE[layer_index], )\n",
    "    import tensorflow as tf\n",
    "    logits = rnn_nodes(X, weights, biases, NUM_HIDDEN_RANGE[layer_index])\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE_RANGE[learning_index])\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # assign vars to default value\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # start tf training\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "\n",
    "        # run the initializer\n",
    "        sess.run(init)\n",
    "\n",
    "        for step in range(1, training_steps + 1):\n",
    "            batch_x, batch_y = render_batch(BATCH_SIZE_RANGE[batch_index], train_x, train_y)\n",
    "            # reshape data to get 100 seq of 3 elements (y,p,r)\n",
    "            batch_x = batch_x.reshape((BATCH_SIZE_RANGE[batch_index], timesteps, num_input))\n",
    "            # run optimization operation by using backprop\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            if step % display_step == 0 or step == 1:\n",
    "\n",
    "                # calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})\n",
    "                training_step_log = \", Step, \" + str(step) + \", Minibatch Loss, \" + \"{:.5f}\".format(loss) + \", Training Accuracy, \" + \"{:.5f}\".format(acc)\n",
    "                LOGGER.debug(\"exp id, \" + str(exp_id) + training_step_log)\n",
    "\n",
    "        LOGGER.debug(\"exp id, \" + str(exp_id) + \", Optimization Finished!\")\n",
    "\n",
    "        # calculate accuracy on testing set\n",
    "        test_data = test_x.reshape((-1, timesteps, num_input))\n",
    "        test_label = test_y\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: test_data, Y: test_label})\n",
    "        test_log = \", Testing Accuracy, \" + str(test_acc)\n",
    "        LOGGER.debug(\"exp id, \" + str(exp_id) + test_log)\n",
    "\n",
    "        train_data = train_x.reshape((-1, timesteps, num_input))\n",
    "        train_label = train_y\n",
    "        train_acc = sess.run(accuracy, feed_dict={X: train_data, Y: train_label})\n",
    "        train_log = \", Training Accuracy, \" + str(train_acc)\n",
    "\n",
    "        LOGGER.debug(\"exp id, \" + str(exp_id) + train_log)\n",
    "        return \"exp id, \" + str(exp_id) + (train_log + test_log)\n",
    "\n",
    "\n",
    "def rnn_training_master(train_x, train_y, test_x, test_y, is_simple_run=False):\n",
    "    import tensorflow as tf\n",
    "    rnn_results = []\n",
    "    exp_id = 0\n",
    "\n",
    "    num_layer_length = len(NUM_HIDDEN_RANGE)\n",
    "    learning_length = len(LEARNING_RATE_RANGE)\n",
    "    batch_length = len(BATCH_SIZE_RANGE)\n",
    "\n",
    "    if is_simple_run:\n",
    "        num_layer_length = SIMPLE_HIDDEN_LEN\n",
    "        learning_length = SIMPLE_LEARNING_LEN\n",
    "        batch_length = SIMPLE_BATCH_LEN\n",
    "\n",
    "    exp_params_queue = Queue.Queue()\n",
    "    for layer_index in range(num_layer_length):\n",
    "        for learning_index in range(learning_length):\n",
    "            for batch_index in range(batch_length):\n",
    "                exp_params_queue.put((layer_index, learning_index, batch_index))\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    while not exp_params_queue.empty():\n",
    "        flush_mt_counter = THREAD_COUNT\n",
    "        rnn_results_temp = []\n",
    "        rnn_pool = multiprocessing.Pool(processes=THREAD_COUNT)\n",
    "\n",
    "        # while flush_mt_counter > 1 and not exp_params_queue.empty():\n",
    "        while flush_mt_counter > 0 and not exp_params_queue.empty():\n",
    "            flush_mt_counter -= 1\n",
    "\n",
    "            current_exp_params = exp_params_queue.get()\n",
    "            sample_result = rnn_pool.apply_async(rnn_training_engine_worker, (exp_id, train_x, train_y, test_x, test_y, current_exp_params[0], current_exp_params[1], current_exp_params[2],))\n",
    "            exp_id += 1\n",
    "            rnn_results_temp.append(sample_result)\n",
    "\n",
    "        rnn_pool.close()\n",
    "        rnn_pool.join()\n",
    "\n",
    "        for sample_result in rnn_results_temp:\n",
    "            rnn_results.append(sample_result)\n",
    "\n",
    "        LOGGER.debug(\"Flush RNN training master\\n\" + CUTOFF_LINE)\n",
    "\n",
    "    rnn_results = transform_apply_result(rnn_results)\n",
    "    return rnn_results\n",
    "\n",
    "\n",
    "def transform_apply_result(input_list):\n",
    "    result_list = []\n",
    "\n",
    "    for i in range(len(input_list)):\n",
    "        result_list.append(input_list[i].get())\n",
    "\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def main():\n",
    "    LOGGER.debug(\"Start reading formatted input\")\n",
    "    train_x, train_y, test_x, test_y = render_raw_data()\n",
    "    LOGGER.debug(\"End reading formatted input\\n\" + CUTOFF_LINE)\n",
    "\n",
    "    LOGGER.debug(\"Start RNN training master\")\n",
    "    master_res = rnn_training_master(train_x, train_y, test_x, test_y, is_simple_run=ENABLE_SIMPLE_RUN)\n",
    "    LOGGER.debug(\"End RNN training master\\n\" + CUTOFF_LINE)\n",
    "\n",
    "    LOGGER.debug(\"Start Exp summary report\")\n",
    "    for res in master_res:\n",
    "        LOGGER.debug(res)\n",
    "\n",
    "    LOGGER.debug(\"End Exp summary report\\n\" + CUTOFF_LINE)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
